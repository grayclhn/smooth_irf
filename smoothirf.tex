\documentclass[12pt,fleqn]{article}
\usepackage{
    amsfonts,
    amsmath,
    amssymb,
    amsthm,
    booktabs,
    enumerate,
    flexisym,
    graphicx,
    setspace,
    slantsc,
    tabularx,
    url,
}

\title{Smooth interpolation for Impulse Response Functions from \allcaps{svar}s}
\author{Gray Calhoun}
\input{VERSION}
\input{latex_misc/abbrevs}

\urlstyle{same}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\usepackage[small]{caption}
\usepackage[T1]{fontenc}
\usepackage[margin=1.25in]{geometry}
\usepackage[charter]{mathdesign}
\usepackage[letterspace=35]{microtype}
\usepackage[sort,round,comma]{natbib}
\bibliographystyle{abbrvnat}
\newcommand\citepos[2][]{\citeauthor{#2}'s \citeyearpar[#1]{#2}}
\newcommand\poscw{\citeauthor{ClW:06}'s \citeyearpar{ClW:06,ClW:07}}
\newcommand\citen[1]{\citeauthor{#1}, \citeyear{#1}}

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\DeclareMathOperator{\whitenoise}{\mathit{white\ noise}}
\newcommand{\vep}{\varepsilon}
\newcommand{\AR}{\allcaps{AR}}
\newcommand{\RR}{\mathbb{R}}

\begin{document}
\maketitle

Impulse Response Functions (\IRF s) are used in signal processing and
time-series analysis to represent the effect over time of a short
impulse or shock on a dynamic system; and they are heavily used in
Macroeconomics to characterize the effects of a shock of theoretical
interest --- the effect of a monetary policy shock, for example, or a
technology shock. Formally, if $y_t$ is a stochastic process with the
\MA($\infty$) representation
\begin{equation*}
  y_t = \sum_{j=0}^{\infty} \psi_j \vep_{t-j}
\end{equation*}
and $\vep_t \sim \whitenoise(0,\Sigma)$ where $\Sigma$ is normalized
to have diagonal elements equal to 1, the \IRF\ is the function
\begin{equation}\label{eq:1}
  \Psi(j) = \theta_j u
\end{equation}
for $j = 0, 1, 2,\dots$ where the vector $u$ has norm 1 and represents
a specific theoretical shock of interest. In Macroeconomics,
researchers usually don't work with \MA\ representations directly, but
instead model the processes as a finite lag \VAR, but the principle is
exactly the same.

An advantage of using \IRF s is that they show the full set of
dynamics implied by a particular parameter estimate of the \VAR, and
to make these dynamics more clear the values determined by
Equation~\eqref{eq:1} are typicially plotted as a graph and joined by
straight lines --- see the left column of Figure~\ref{f1} for several
examples. Indeed, these graphs and connecting lines are indespensible
when several \IRF s are plotted in the same graph to represent
statistical uncertainty or a set of potential dynamics --- without
some method of connecting the points, we can see the marginal
distributions of each estimate of $\Psi(j)$, but can not see their
joint distributions.

Unfortunately, the ubiquitous method of connecting the points
$\Psi(0), \Psi(1),\dots$, i.e.\ drawing a straight line between them,
can introduce misleading visual distortions, which we will explicitly
discuss later. In this paper, we show how the \IRF\ $\Psi(j)$ can be
meaningfully defined for \VAR s for any real positive value of $j$ and
give a convenient method for calculating it. (Figures~\ref{f1}
and~\ref{f2} have examples if you're impatient to see them.) This
allows researchers to use the estimated \VAR\ itself to interpolate
between the points for which it is usually defined and results in
graphs that do not have the distortions produced by linear
interpolation. Moreover, using the smoothed \IRF\ also allows for
refinements to partial identification strategies based on sign
restrictions.

\section{Motivation and theory}

\begin{figure}[t]
  \centering
  \includegraphics{graphs/motivation.pdf}
  \caption{\IRF\ for simple AR(1) example. Here $y_t = 0.2 y_{t-1} +
    \vep_t$. The left panel plots the standard \IRF, using linear
    interpolation between the points (0,1), (1, .1), (1, .01), etc.,
    while the right panel plots the function $0.2^x$ directly.}
  \label{f0}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics{graphs/motivation2.pdf}
  \caption{\IRF\ for simple AR(1) example. Here $y_t = -0.2 y_{t-1} +
    \vep_t$. The left panel plots the standard \IRF, using linear
    interpolation between the points (0,1), (1, .1), (1, .01), etc.,
    while the right panel plots the function $- 0.2^x \cos(\pi x)$ directly.}
  \label{f0b}
\end{figure}

Start with the simplest univariate case and suppose that
\begin{equation}\label{eq:2}
y_t = a y_{t-1} + \vep_t
\end{equation}
where $a \in [0, 1)$, which can be rewritten as
\begin{equation}
  \label{eq:3}
  y_t = y_0 + \sum_{j=0}^{t-1} a^j \vep_{t-j},
\end{equation}
giving the \IRF\ $\Psi(j) = a^j u$. This function is obviously
well-defined for all positive real $j$, and can be used directly to
interpolate between the integer points. Figure~\ref{f0} plots the
standard \IRF\ (using linear interpolation) on the left and the
function $a^j u$ on the right. While one could debate which of the
functions represents the ``true'' impact at a fractional point in
time, the right panel more clearly expresses the shock's rate of
decay.

Also note that $\Psi(j) = a^j u$ satisfies the recurrence relation
implied by the lag structure of the original \AR\ process for any
positive real value of $j$:
\begin{equation*}
  \Psi(j) = a^j u = a \times a^{j-1} u = a \Psi(j-1).
\end{equation*}
It seems natural to require that any smooth function $\Psi^*$ that we
use to represent \IRF s satisfy two conditions,
\begin{enumerate}
\item $\Psi^*(j) = \Psi(j)$ (the true \IRF) for all integers $j$
\item $\Psi^*(j)$ satisfy the recurrence relation defined by the lag
  structure of the underlying \VAR\ process; i.e.\ if $\Phi(L)$ is the
  lag polynomial of the \VAR, so $\Phi(L)y_t = \vep_t$ for $t = 1, 2,
  \dots$, then $\Phi(L) \Psi^*(t) = 0$ for all $t \in \RR$.
\end{enumerate}

Now suppose that $a \in (-1, 0)$. Again
\begin{equation}
  \label{eq:4}
    y_t = y_0 + \sum_{j=0}^{t-1} a^j \vep_{t-j},
\end{equation}
and the \IRF\ $\Psi(j) = a^j$ is well-defined and has a real value for
all integer values of $j$, but may not be real for non-integer values
of $j$. However, for any integer value of $j$ the equality
\begin{equation}
  \label{eq:5}
  a^j = |a|^j \cos(\pi j)
\end{equation}
holds, and the function $\Psi(j) = |a|^j \cos(\pi j) u$ satisfies the
recurrence relation implied by the \AR\ model's lag structure when $a < 0$:
\begin{equation*}
  \Psi(j) = |a|^j \cos(\pi j) \cdot u = - |a| \times |a|^{j-1} \cos(\pi (j-1)) \cdot u = a \Psi(j-1).
\end{equation*}
So, since it is exactly correct on the integers and satisfies the
model's dynamics for noninteger values as well, this choice of $\Psi$
is a sensible choice for interpolating between the
integers. Figure~\ref{f0b} plots both functions, and again the smooth
version more accurately reflects the system's dynamics.

This choice of $\Psi(j)$ was not made at random. For any complex
number $a + bi$, we can write it in polar form:
\[
a + bi = R [\cos(\theta) - i \sin(\theta)]
\]
where $R = |a|$ and $\theta$ satisfies $\cos(\theta) = a/R$ and
$\sin(\theta) = b/R$; then real powers can be defined easily as
\[
(a + bi)^j = R^j [\cos(\theta j) - i \sin(\theta j)].
\]
For our \AR\ coefficient $a < 0$, we have $\theta = \pi$, giving
$a = |a| \cos(\pi)$ as we originally claimed.

In general,%
\footnote{See Chapter 1 of \citet{Ham:94} for a textbook treatment.} %
any \VAR($p$) has a \VAR(1) ``canonical'' representation. If
\[
y_t = \sum_{i=1}^p A_i y_{t-i} + \vep_t
\]
then
\[
\underbrace{\begin{pmatrix}
  y_t \\ y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p+1}
\end{pmatrix}}_{Z_t}
=
\underbrace{\begin{pmatrix}
  A_1 & A_2 & \cdots & A_{p-1} & A_p \\
  I_k & 0   & \cdots & 0 & 0 \\
  0  & I_k  & \cdots & 0 & 0 \\
  \vdots & \vdots & & \vdots & \vdots \\
  0 & 0 & \cdots & I_k & 0 
\end{pmatrix}}_{F}
\underbrace{\begin{pmatrix}
  y_{t-1} \\ y_{t-2} \\ y_{t-3} \\ \vdots \\ y_{t-p}
\end{pmatrix}}_{Z_{t-1}}
+
\underbrace{\begin{pmatrix}
  \vep_{t} \\ 0 \\ 0 \\ \vdots \\ 0
\end{pmatrix}}_{U_t}.
\]
Moreover, if the matrix $F$ is $s \times s$ with $q$ distinct
eigenvalues, then it can be decomposed as
\[
F = M J M^{-1}
\]
where $J$ is block diagonal of the form
\[
J = \begin{pmatrix}
  J_1 & 0 & \cdots & 0 \\
  0 & J_2 & \cdots & 0 \\
  \vdots & \vdots & \cdots & \vdots \\
  0 & 0 & \cdots & J_q
\end{pmatrix}
\]
and each $J_i$ has the form
\[
J_i =
\begin{pmatrix}
  \lambda_i & 1 & 0 & \cdots & 0 & 0 \\
  0 & \lambda_i & 1 & \cdots & 0 & 0 \\
  0 & 0 & \lambda_i & \cdots & 0 & 0 \\
  \vdots & \vdots & \vdots & \cdots & \vdots & \vdots \\
  0 & 0 & 0 & \cdots & \lambda_i & 1 \\
  0 & 0 & 0 & \cdots & 0 & \lambda_i
\end{pmatrix}
\]
where $\lambda_1,\dots,\lambda_q$ are the distinct eigenvalues of $F$.
If all of the eigenvalues of $F$ are distinct, this is just the
eigenvalue decomposition of $F$.

This representation is convenient because, for positive integer values
of $t$, we have
\[
F^t = M J^t M^{-1}
\]
and $J^t$ has the convenient form
\begin{align*}
J^t           & =
\begin{pmatrix}
  J_1^t       & 0                      & \cdots                 & 0                                                                        \\
  0           & J_2^t                  & \cdots                 & 0                                                                        \\
  \vdots      & \vdots                 & \cdots                 & \vdots                                                                   \\
  0           & 0                      & \cdots                 & J_q^t
\end{pmatrix},
\intertext{where}
J_i^t         & =
\begin{pmatrix}
  \lambda_i^t & \binom{t}{1} \lambda_i^{t-1} & \binom{t}{2} \lambda_i^{t-2} & \cdots & \binom{t}{m_i} \lambda_i^{t-m_i}     \\
  0           & \lambda_i^t            & \binom{t}{1} \lambda_i^{t-1} & \cdots & \binom{t}{m_i-1} \lambda_i^{t-m_i+1} \\
  0           & 0                      & \lambda_i^t            & \cdots & \binom{t}{m_i-2} \lambda_i^{t-m_i+2} \\
  \vdots      & \vdots                 & \vdots                 & \cdots & \vdots                         \\
  0           & 0                      & 0                      & \cdots & \lambda_i^t \\
\end{pmatrix} \\
&=
\begin{pmatrix}
  h_i(t,0) & h_i(t,1) & h_i(t,2) & \cdots & h_i(t,m_i)   \\
  0        & h_i(t,0) & h_i(t,1) & \cdots & h_i(t,m_i-1) \\
  0        & 0        & h_i(t,0) & \cdots & h_i(t,m_i-2) \\
  \vdots   & \vdots   & \vdots   & \cdots & \vdots      \\
  0        & 0        & 0        & \cdots & h_i(t,0)    \\
\end{pmatrix} \\
\intertext{and}
h_j(t,s) &=
\begin{cases}
  |\lambda_j|^t \big[\cos(\theta_j t) + i \sin(\theta_j t)\big] & s = 0 \\
  \frac{t!}{j!(t-s)!} |\lambda_j|^t \big[\cos(\theta_j t) + i \sin(\theta_j t)\big] & \text{if } t \geq s > 0 \\
  0 & \text{otherwise.}
\end{cases}
\end{align*}
where $\cos(\theta_j) = \Re(\lambda_j)/|\lambda_j|$ and
$\sin(\theta_j) = \Im(\lambda_j)/|\lambda_j|$ as above.

Then we can write
\begin{multline}\label{eq:6}
  F^t =
  \sum_{j = 1}^q \sum_{l=0}^{m_i} \tfrac{t(t-1)\cdots(t-l+1)}{l (l - 1) \cdots 1} |\lambda_j|^{t-l} \\
  \times\Big\{\big[A_{jl} \cos(\theta_j t) + B_{jl}(t) \sin(\theta_j t)\big] + i\big[C_{jl} \cos(\theta_j t) + D_{jl}(t) \sin(\theta_j t)\big]\Big\}
\end{multline}
where the coefficient matrices $A_{jl}$, $B_{jl}$, $C_{jl}$, and
$D_{jl}$ are chosen to match $F^0, F^1, F^2,\dots$. 
For integer values of $t$, the imaginary components of $M J^t M^{-1}$
exactly cancel, so
\begin{equation*}
  0 = \sum_{j = 1}^q \sum_{l=0}^{m_i} \tfrac{t(t-1)\cdots(t-l+1)}{l (l - 1) \cdots 1} |\lambda_j|^{t-l}
  \big[C_{jl} \cos(\theta_j t) + D_{jl}(t) \sin(\theta_j t)\big]
\end{equation*}
for $t = 1,2, \dots$ and we can use the real part alone
\begin{equation}\label{eq:7}
  F^t =
  \sum_{j = 1}^q \sum_{l=0}^{m_i} \tfrac{t(t-1)\cdots(t-l+1)}{l (l - 1) \cdots 1} |\lambda_j|^{t-l} \big[A_{jl} \cos(\theta_j t) + B_{jl}(t) \sin(\theta_j t)\big]
\end{equation}
to produce $F^1, F^2, F^3,\dots$. Our proposal, in a nushell, is to
use the same function for all of the reals rather than just the
integers, exactly as we did in the motivating \AR(1) examples.

Equation~\eqref{eq:6} has another practical implication. Although
$F^t$ has real and imaginary components for real $t$, we are only
interested in its real component, which genereates the \IRF s for
integer values of $t$. So rather than working out the matrices
$A_{jl}$ and $B_{jl}$ to use~\eqref{eq:7}, we can calculate the
complex valued $F^t$ and drop its imaginary component. This
calculation is directly available in many programming languages and
can be implemented as $M J^t M^{-1}$ using the Jordan decomposition if
it is not.

Intuitively, we can see that the real component of $F^t$ must agree
with the original \IRF s on the integer values and that $\Re(F^t)$
also satisfies the original \VAR's recurrence relations. If $a_t$ is
any real sequence that satisfies $F a_{t-1} = a_t$ for the real matrix
$F$, then $a_t' = \Re(F^t) a_{0} \equiv \Re(M J^t M^{-1}) a_0$
trivially satisfies the same recurrence relationship. Since $F$ is
real, $F \Re(F^{t-1}) = \Re(F^t)$ and we have
\begin{align*}
F a'_{t-1} &= F \Re(F^{t-1}) a_{0} \\
&= \Re(F^t) a_0 \\
&= a'_t.
\end{align*}

\section{Numerical example}

For a numerical example, consider the two-variable \VAR(2)
\begin{equation}
  \begin{pmatrix}
    y_{1t} \\ y_{2t}
  \end{pmatrix}
  =
  \begin{pmatrix}
    - 0.5 & 0.01 \\ 0.3 & 0.1
  \end{pmatrix}
  \begin{pmatrix}
    y_{1,t-1} \\ y_{2,t-1}
  \end{pmatrix}
  +
  \begin{pmatrix}
    -0.2 & 0.1 \\ -0.1 & 0.0
  \end{pmatrix}
  \begin{pmatrix}
    y_{1,t-2} \\ y_{2,t-2}
  \end{pmatrix}
  +
  \begin{pmatrix}
    \vep_{1t} \\ \vep_{2t}
  \end{pmatrix}
\end{equation}
and assume for the sake of argument that $(\vep_{1t},\vep_{2t}) \sim
N(0,I)$ represents the shocks of interest.

As described above, we will define the smooth \IRF s for a shock to
the first element as
\begin{equation*}
  \begin{pmatrix}
    \hat y_{1j} \\ \hat y_{2j}
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0
  \end{pmatrix}
  V D^j V^{-1}
  \begin{pmatrix}
    1 \\ 0 \\ 0 \\ 0
  \end{pmatrix}
\end{equation*}
with
\begin{equation}
V = \begin{pmatrix}
     0.23 + 0.30 i & 0.23 - 0.30 i  & 0.06 + 0.03 i & 0.06 - 0.03 i \\
     0.27 + 0.06 i & 0.27 - 0.06 i  & 0.12 + 0.13 i & 0.12 - 0.13 i \\
     0.71 + 0.00 i & 0.71 + 0.00 i  & 0.31 - 0.12 i & 0.31 + 0.12 i \\
    -0.23 - 0.47 i & -0.23 + 0.47 i & 0.92 + 0.00 i & 0.92 + 0.00 i
  \end{pmatrix}
\end{equation}
and
\begin{equation}
  D = diag(-0.33 + 0.42 i, -0.33 - 0.42 i, 0.13 + 0.14 i, 0.13 - 0.14 i)
\end{equation}
the matrices of eigenvectors and eigenvalues of
\begin{equation*}
  \begin{pmatrix}
   -0.5            &  0.01          & -0.2          & 0.1          \\
    0.3            &  0.1           & -0.1          & 0.0          \\
    1.0            &  0.0           &  0.0          & 0.0          \\
    0.0            &  1.0           &  0.0          & 0.0
  \end{pmatrix}.
\end{equation*}
The same equations hold for the \IRF s for a shock to the second
element of $y_t$, but now the last vector should be $(0, 1, 0, 0)'$.

\begin{figure}[t]
  \centering
  \includegraphics{graphs/numeric.pdf}
  \caption{Impulse Response Functions from example}
  \label{f1}
\end{figure}

We plot the \IRF s in Figure~\ref{f1}. The first column plots the
standard \IRF s and the second plots our proposed smooth
plots. Although the general impressions from both graphs are similar,
there are important differences. First, peaks and troughs are often
underestimated by the standard IRFs and their timing is frequently
misidentified. This is especially apparent in the first peak in the
second row. Second, even the sign of the \IRF\ can be misidentified,
as we see with the immediate response of $y_{1t}$ to a shock in
$y_{2t}$. Finally, unless turning points in the smooth functions
happen to coincide with integer values, discretizing the \IRF s
introduces misleading asymmetries and kinks.

\begin{figure}[t]
  \centering
  \includegraphics{graphs/numeric2.pdf}
  \caption{Impulse Response Functions from example}
  \label{f2}
\end{figure}

These distortions are even more apparent when we graph multiple
perterbations of the \IRF s in the same panel, which is a common way
of representing uncertainty or set-identified responses. To
demonstrate this phenomenon, we generated 150 replications of the \IRF
s by adding independent N(0, 0.15) noise terms to each element of $A$,
and then plotting the implied \IRF s as before.

These graphs are shown in Figure~\ref{f2}. The same issues apparent in
Figure~\ref{f1} are present here as well. But there are other problems
as well. In the first curve, for example, the discrete \IRF\ shows
that there is substantial negative correlation between the period 2
and period 3 estimated response and the period 3 and 4 response. But
the smoothed graph makes it clear that this is driven by the timing
and size of the first peak. When that peak is near period 2, the curve
has time to fall noticably before period 2, but when the peak is
closer to period 3, the curve is still rising for that interval. The
actual dynamics implied by the different curves are very
similar. Similar but less dramatic distortions appear in the other
panels as well. In the third row, for example, the discrete \IRF\
shows that about half of the parameter values have an initial increase
in response to a $y_{20}$ shock and half have an initial decrease, but
the smooth curves show that virtually all of them have an immediate
decrease, but that many start to increase very soon. The exact
location of the peak that falls between periods 1 and 2 determines
most of the initial dynamics, but this is impossible to see in the
discrete curve.

\section{Discussion}

Vector Autoregressive models do not just have implications for the
period-to-period dynamics of a stochastic process, they also have
implications about the very short-run dynamics within periods. In this
paper, we propose that researchers graph those intra-period dynamics
when plotting \IRF s and we give a simple method to do so, based on
the \VAR's canonical \VAR(1) representation. Even when researchers do
not want to assign any economic importance to these ultra short-run
dynamics, plotting them in the \IRF s minimizes visual distortions
that can arise from discretizing the dynamics, especially when several
\IRF s are plotted over eachother to represent uncertainty or set
identification --- see Figure~\ref{f2} for an example. If researchers
are willing to interpret these ultra short-run dynamics as having an
economic interpretation, they can also be used to refine sign
restrictions used in partial identification.

\bibliography{localrefs}
\end{document}
